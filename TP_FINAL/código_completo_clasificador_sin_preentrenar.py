# -*- coding: utf-8 -*-
"""Código_completo_clasificador_sin_preentrenar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144p3KVSAoZus-LH-Zut0GetWwSC_4cmR

# CLASIFICADOR SIN PRE-ENTRENAR
"""

# 1.1)
import os
import pickle
import datetime
from collections import defaultdict
# 1.2)
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np
import scipy as sp
import scipy.linalg as linalg
import sklearn as skl
import pandas as pd
#import dill
import json
# 1.3)
import torch
import torch.optim as optim
from torch import nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader, Subset, random_split
from torchvision import datasets
from torchvision import transforms
from torchvision.io import read_image
from torchvision.transforms import ToTensor, Lambda, Compose
#from torchviz import make_dot
# 1.4)
import google.colab
from google.colab import files
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 2.1)
# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])

# Download and load the training data
train_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = True,  transform = transform)
valid_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)
class CustomDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)
    def __getitem__(self,i):
        image, label = self.dataset[i]
        input = image
        output = image
        return input, output
#Convertimos Fashion Mnist Dataset a CustomDataset

train_set = CustomDataset(train_set_orig)
valid_set = CustomDataset(valid_set_orig)

# 2.2)
figure = plt.figure()
cols,rows = 3,3
for i in range(1,cols*rows+1):
    j = torch.randint(len(train_set_orig),size=(1,)).item() # Los números aleatorios tambien se pueden generar desde pytorch. Util para trabajar en la GPU.
    image,label = train_set_orig[j]
    figure.add_subplot(rows,cols,i)
    #plt.title(labels_names[label])
    plt.axis("off")
    plt.imshow(image.squeeze(),cmap="Greys_r")
plt.show()



#2.4

#Creo una instancia de una funcion de perdida, en este caso una entropy losso MSE

#loss_fn = nn.CrossEntropyLoss()
loss_fn = nn.MSELoss() #Para el autoencoder

"""# CLASIFICADOR"""

labels_names = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot"
}

#train y valid loop

def train_loop(dataloader, model, loss_fn, optimizer, verbose = True):
    model.train()  # Activamos el modo de entrenamiento
    num_samples = len(dataloader.dataset)
    num_batches = len(dataloader)
    sum_batch_avrg_loss = 0
    sum_correct = 0            #CAMBIO ACÁ
    num_processed_samples = 0


    #model = model.to(device)   # ESTO CAMBIE
    #Iteramos sobre los lotes
    for batch, (X,y) in enumerate(dataloader):
      X = X.to(device)
      y = y.to(device)
      batch_size = len(X)
      num_processed_samples += batch_size

      #Calculamos la prediccion del modelo y la funcion de perdida correspondiente
      pred = model(X)
      loss = loss_fn(pred,y)

      #Backpropagamos usando el optimizador atribuido
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      #Calculamos la perdida promedio del batch y lo agregamos a una suma correspondiente
      batch_avrg_loss = loss.item()
      sum_batch_avrg_loss += batch_avrg_loss
      #Numero de predicciones correctas:
      sum_correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      #Reportamos el proceso
      if batch % (num_batches/10) == 0 and verbose:
        current = batch * len(X)
        print(f"loss: {batch_avrg_loss:>7f}  [{current:>5d}/{num_samples:>5d}]")

    avg_loss = sum_batch_avrg_loss/num_batches
    precision = sum_correct/num_samples
    return avg_loss, precision

def valid_loop(dataloader, model, loss_fn):
    # Desactivamos la maquinaria de entrenamiento del modelo
    model.eval()

    # Definimos ciertas constantes
    num_samples = len(dataloader.dataset)
    num_batches = len(dataloader)
    sum_batch_avrg_loss = 0
    sum_correct = 0
    num_processed_samples = 0

    # Movemos el modelo a la GPU si es que está disponible
    #model = model.to(device)                      #NO ESTÁ EN EL CODIGO

    # Para testear, desactivamos el cálculo de gradientes.
    with torch.no_grad():
        # Iteramos sobre lotes (batches)
        for X, y in dataloader:
            # Copiamos las entradas y las salidas al dispositivo de trabajo si es que está disponible
            X = X.to(device)
            y = y.to(device)
            batch_size = len(X)  # Número de muestras en el lote
            num_processed_samples += batch_size

            # Calculamos las predicciones del modelo...
            pred = model(X)
            loss = loss_fn(pred, y)

            # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente.
            batch_avrg_loss = loss.item()
            sum_batch_avrg_loss += batch_avrg_loss
            sum_correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        # Calculamos la pérdida total promedio dividiendo entre el número de batches
        avg_loss = sum_batch_avrg_loss / num_batches
        precision = sum_correct/num_samples

        # Imprimimos el resultado (opcional)
        # print(f"@eval_loop avg_loss={avg_loss:>.8f}")

    return avg_loss, precision
#En el codigo del profe utiliza frac_correct



import torch
import torch.nn as nn

class Clasificador_Conv(nn.Module):
    def __init__(self, n1=128, n2=64, p=0.2):
        super().__init__()

        self.n1 = n1
        self.n2 = n2
        self.p = p

        print("Creating encoder...")

        # **Definir el encoder directamente en el clasificador**
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(self.p),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(self.p),
            nn.MaxPool2d(2, 2),

            nn.Flatten()  # Aplanar salida antes de la capa lineal
        )

        # **Clasificador con 3 capas densas**
        self.clasificador = nn.Sequential(
            nn.Linear(32 * 7 * 7, self.n1),
            nn.ReLU(),
            nn.Dropout(self.p),

            nn.Linear(self.n1, self.n2),
            nn.ReLU(),
            nn.Dropout(self.p),

            nn.Linear(self.n2, 10)  # 10 clases de salida
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.clasificador(x)
        return x

classificador_conv = Clasificador_Conv()
model=classificador_conv

def batch(x):
  return x.to(device).unsqueeze(0)

def unbatch(x):
  return x.squeeze().detach().cpu().numpy()



model = model.to(device) ###################### CON ESTO LO ARREGLE
image, label = train_set_orig[1]
pred = model(batch(image))

batch_size = 128

# Define los DataLoaders correctamente
train_loader = DataLoader(train_set_orig, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_set_orig, batch_size=batch_size, shuffle=True)

# Define la función de pérdida
loss_fn = nn.CrossEntropyLoss()
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-08, weight_decay=0, amsgrad=False)



# SI QUISIERAMOS ENTRENAR TODO, INCLUIDO EL ENCODER, LA FUNCION SERÍA LA SIGUIENTE:
# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-08, weight_decay=0, amsgrad=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)     #VER ESTO

###### REVISAR
# Inicializamos listas para guardar métricas

list_train_avg_loss_incorrecta = []
list_train_avg_loss = []
list_valid_avg_loss = []
list_train_precision_incorrecta = []
list_train_precision = []
list_valid_precision = []

# Ciclo de entrenamiento
num_epochs = 30
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}\n-------------------------------")

    # Entrenamos y validamos
    train_avg_loss_incorrecta, train_precision_incorrecta = train_loop(train_loader, model, loss_fn, optimizer)
    train_avg_loss, train_precision = valid_loop(train_loader, model, loss_fn)
    valid_avg_loss, valid_precision = valid_loop(valid_loader, model, loss_fn)

    # Guardamos las métricas
    list_train_avg_loss_incorrecta.append(train_avg_loss_incorrecta)
    list_train_avg_loss.append(train_avg_loss)
    list_valid_avg_loss.append(valid_avg_loss)
    list_train_precision_incorrecta.append(train_precision_incorrecta)
    list_train_precision.append(train_precision)
    list_valid_precision.append(valid_precision)


    print(f"Train Loss: {train_avg_loss:.4f}, Train Accuracy: {train_precision:.4f}")
    print(f"Valid Loss: {valid_avg_loss:.4f}, Valid Accuracy: {valid_precision:.4f}")

print("Entrenamiento finalizado!")

min_train_loss_epoch = list_train_avg_loss.index(min(list_train_avg_loss)) + 1
max_train_precision_epoch = list_train_precision.index(max(list_train_precision)) + 1
min_valid_loss_epoch = list_valid_avg_loss.index(min(list_valid_avg_loss)) + 1
max_valid_precision_epoch = list_valid_precision.index(max(list_valid_precision)) + 1

min_train_loss = min(list_train_avg_loss)
max_train_precision = max(list_train_precision)
min_valid_loss = min(list_valid_avg_loss)
max_valid_precision = max(list_valid_precision)

# Imprimir resultados
print("\n--- Resultados finales ---")
print(f"Epoch con mínima pérdida de entrenamiento: {min_train_loss_epoch}, Pérdida: {min_train_loss:.4f}")
print(f"Epoch con máxima precisión de entrenamiento: {max_train_precision_epoch}, Precisión: {max_train_precision:.4f}")
print(f"Epoch con mínima pérdida de validación: {min_valid_loss_epoch}, Pérdida: {min_valid_loss:.4f}")
print(f"Epoch con máxima precisión de validación: {max_valid_precision_epoch}, Precisión: {max_valid_precision:.4f}")



import matplotlib.pyplot as plt

# Graficar pérdida
plt.figure(figsize=(12, 6))

# Pérdida de entrenamiento
#plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), list_train_avg_loss, label='Entrenamiento', color='blue')
plt.plot(range(num_epochs), list_valid_avg_loss, label='Validación', color='green')
plt.plot(range(num_epochs), list_train_avg_loss_incorrecta, label='Durante la época', color='red', linestyle='--')
#plt.title('Pérdida durante el entrenamiento')
plt.xlabel('Época')
plt.ylabel('Pérdida')
plt.legend()

# Precisión de entrenamiento
#plt.subplot(1, 2, 2)
plt.plot(range(num_epochs), list_train_precision, label='Entrenamiento', color='blue')
plt.plot(range(num_epochs), list_valid_precision, label='Validación', color='green')
plt.plot(range(num_epochs), list_train_precision_incorrecta, label='Durante la época', color='red', linestyle='--')
#plt.title('Precisión durante el entrenamiento')
plt.xlabel('Época')
plt.ylabel('Precisión')
plt.legend()

plt.tight_layout()
plt.show()

import torch
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt

# Inicializamos listas para las etiquetas verdaderas y predicciones
y_pred = []
y_true = []

# Iteramos sobre los datos de validación
size = len(valid_loader.dataset)
for batch, (inputs, labels) in enumerate(valid_loader):
    inputs = inputs.to(device)  # Movemos las entradas al dispositivo (GPU/CPU)
    labels = labels.to(device)  # Movemos las etiquetas al dispositivo

    # Hacemos la predicción
    outputs = model(inputs)  # Paso hacia adelante por la red

    # Obtenemos las predicciones
    _, predicted = torch.max(outputs, 1)
    y_pred.extend(predicted.cpu().numpy())  # Convertir a numpy y agregar a y_pred

    y_true.extend(labels.cpu().numpy())  # Convertir a numpy y agregar a y_true

    if batch % 10 == 0:
        current = batch * len(inputs)
        print(f"batch={batch:>5d} muestras-procesadas:{current:>5d}/{size:>5d}")

# Definimos las clases
classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']

# Construimos la matriz de confusión
cf_matrix = confusion_matrix(y_true, y_pred)

# Normalizamos la matriz de confusión por las filas (para que cada fila sume 1)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None],
                     index=classes,
                     columns=classes)

# Graficamos la matriz de confusión
plt.figure(figsize=(12, 7))
sn.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues', cbar=True)
#plt.title('Matriz de Confusión Normalizada')
plt.ylabel('Clase Real')
plt.xlabel('Clase Predicha')
plt.show()

import torch
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt

# Inicializamos listas para las etiquetas verdaderas y predicciones
y_pred = []
y_true = []

# Iteramos sobre los datos de validación
size = len(valid_loader.dataset)
for batch, (inputs, labels) in enumerate(valid_loader):
    inputs = inputs.to(device)  # Movemos las entradas al dispositivo (GPU/CPU)
    labels = labels.to(device)  # Movemos las etiquetas al dispositivo

    # Hacemos la predicción
    outputs = model(inputs)  # Paso hacia adelante por la red

    # Obtenemos las predicciones
    _, predicted = torch.max(outputs, 1)
    y_pred.extend(predicted.cpu().numpy())  # Convertir a numpy y agregar a y_pred

    y_true.extend(labels.cpu().numpy())  # Convertir a numpy y agregar a y_true

    if batch % 10 == 0:
        current = batch * len(inputs)
        print(f"batch={batch:>5d} muestras-procesadas:{current:>5d}/{size:>5d}")

# Definimos las clases
classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']

# Construimos la matriz de confusión SIN normalizar
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)

# Graficamos la matriz de confusión
plt.figure(figsize=(12, 7))
sn.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', cbar=True)  # fmt='d' para enteros
plt.ylabel('Clase Real')
plt.xlabel('Clase Predicha')
plt.show()