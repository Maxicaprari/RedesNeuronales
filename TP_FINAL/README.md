# Autoencoder y Clasificador Convolucional para Fashion-MNIST

Este repositorio contiene la implementaci贸n y an谩lisis de un Autoencoder Convolucional y un Clasificador Convolucional aplicados al dataset Fashion-MNIST. El objetivo del proyecto es estudiar c贸mo distintas configuraciones del espacio latente afectan la reconstrucci贸n de im谩genes y evaluar el desempe帽o de encoders pre-entrenados reutilizados en tareas de clasificaci贸n.

# Arquitecturas
**1. Autoencoder Convolucional**

Encoder: Red convolucional con varias capas de convoluci贸n y \textit{max-pooling} para reducir progresivamente la dimensi贸n de las im谩genes y capturar representaciones latentes.

Latent Space: Configurable, lo que permite controlar el nivel de compresi贸n.

Decoder: Red convolucional transpuesta para reconstruir las im谩genes a partir del espacio latente.

**2. Clasificador Convolucional**

Entrenado desde cero: Red convolucional est谩ndar entrenada directamente para la tarea de clasificaci贸n.

Con encoder pre-entrenado: Se reutiliza el encoder del autoencoder para inicializar el clasificador y se comparan los resultados entre diferentes estrategias de entrenamiento (congelando o ajustando los pesos).

# 1. Carpetas principales
**Resultados/**: Contiene gr谩ficos, matrices de confusi贸n y archivos de resultados del entrenamiento y validaci贸n.
# 2. Archivos de c贸digo 

A continuaci贸n, se describen los archivos de c贸digo clave:

**2capas_no_lineal_bn32.py**: Implementaci贸n de una red feedforward con dos capas ocultas y activaciones no lineales (ReLU), tama帽o de lote de 30.

**loops_clasificador.py**: C贸digo con los loops de entrenamiento y validaci贸n para el clasificador.

matriz_de_confusi贸n.py: Genera y visualiza la matriz de confusi贸n para evaluar el desempe帽o del clasificador.

**clasificador_para_autoencoder_1capa_nolineal.py**: Clasificador que utiliza representaciones generadas por un autoencoder de una capa no lineal.

**clasificador_sin_preentrenar_1.py y c贸digo_completo_clasificador_sin_preentrenar.py**: Clasificador desarrollado sin preentrenamiento con autoencoders.

# 3. Arquitecturas de Autoencoders 
**arquitectura_1_autoencoder.py**: Autoencoder b谩sico de una sola capa.

**arquitectura_2_autoencoder(3_capas).py**: Autoencoder m谩s profundo con tres capas ocultas.

**arquitectura_3_autoencoder(1capa).py y arquitectura_3_autoencoder(1capa,_no_lineal).py:** Autoencoders de una capa con activaciones no lineales para mejor representaci贸n de las caracter铆sticas.

**arquitectura_autoencoder_no__lineal_1.py:** Versi贸n alternativa del autoencoder no lineal.

# Resultados

**Clasificador**
**Encoder pre-entrenado**: Ofrece una mejor representaci贸n inicial, acelerando la convergencia en las primeras 茅pocas. Sin embargo, para alcanzar el mejor desempe帽o, fue necesario ajustar el modelo para la tarea espec铆fica de clasificaci贸n.

**Entrenado desde cero**: Aunque requiere m谩s tiempo de entrenamiento, logr贸 mejores resultados cuando se optimiz贸 correctamente.

# Requisitos

Python 3.x

PyTorch

Numpy

Matplotlib
