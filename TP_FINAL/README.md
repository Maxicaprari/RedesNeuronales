# Computational Neuroscience and Deep Learning Projects üß†üíª  
Este repositorio contiene una colecci√≥n de trabajos pr√°cticos desarrollados en el marco de proyectos relacionados con modelos neuronales y aprendizaje profundo (Deep Learning). Cada secci√≥n incluye el c√≥digo, las arquitecturas utilizadas, los loops de entrenamiento y validaci√≥n, as√≠ como los resultados obtenidos y sus respectivos informes.

## Contenido üìÇ  

### Trabajo 1: Modelo de Hodgkin y Huxley  
- Implementaci√≥n del modelo cl√°sico para el an√°lisis del potencial de acci√≥n en neuronas.  
- Resoluci√≥n num√©rica mediante el m√©todo de Runge-Kutta de cuarto orden (RK4).  
- An√°lisis de respuesta neuronal ante diferentes est√≠mulos de corriente y evoluci√≥n de los canales i√≥nicos.  

### Trabajo 2: Clasificaci√≥n con Redes Neuronales Feedforward (MLP)  
- Implementaci√≥n de una red neuronal feedforward para la clasificaci√≥n de im√°genes del dataset Fashion-MNIST.  
- Dise√±o de una arquitectura con dos capas ocultas y funci√≥n de activaci√≥n ReLU.  
- Entrenamiento y evaluaci√≥n utilizando diferentes hiperpar√°metros: learning rate, tama√±o de lote, y optimizadores como Adam y SGD.  
- An√°lisis de la p√©rdida y precisi√≥n en el conjunto de validaci√≥n para comparar el rendimiento de la red.  

### Trabajo Final: Autoencoder y Clasificador Convolucional para Fashion-MNIST  
Este trabajo incluye la implementaci√≥n y an√°lisis de un **Autoencoder Convolucional** y un **Clasificador Convolucional** aplicados al dataset Fashion-MNIST. El objetivo es estudiar c√≥mo distintas configuraciones del espacio latente afectan la reconstrucci√≥n de im√°genes y evaluar el desempe√±o de encoders preentrenados reutilizados en tareas de clasificaci√≥n.  

#### Arquitecturas üîÑ  

##### Autoencoder Convolucional  
- **Encoder:** Red convolucional con varias capas de convoluci√≥n y *max-pooling* para reducir progresivamente la dimensi√≥n de las im√°genes y capturar representaciones latentes.  
- **Latent Space:** Configurable, lo que permite controlar el nivel de compresi√≥n.  
- **Decoder:** Red convolucional transpuesta para reconstruir las im√°genes a partir del espacio latente. *Nota: Los autoencoders no lineales no tienen una capa lineal intermedia, lo que les permite capturar relaciones m√°s complejas entre las variables de entrada.*  

##### Clasificador Convolucional  
- **Entrenado desde cero:** Red convolucional est√°ndar entrenada directamente para la tarea de clasificaci√≥n.  
- **Con encoder pre-entrenado:** Se reutiliza el encoder del autoencoder para inicializar el clasificador. Se realizan comparaciones entre diferentes estrategias de entrenamiento (congelando o ajustando los pesos), utilizando **transfer learning** y evitando **fine-tuning** debido al riesgo de sobreajuste.  

#### Estructura del Proyecto üõ†Ô∏è  
##### Carpetas principales  
- `Resultados/`: Contiene gr√°ficos, matrices de confusi√≥n y archivos de resultados del entrenamiento y validaci√≥n.  
- `Informes/`: Incluye el PDF del trabajo final con los detalles del proyecto.  

##### Archivos de c√≥digo üìù  
- `2capas_no_lineal_bn32.py`: Implementaci√≥n de una red feedforward con dos capas ocultas y activaciones no lineales (ReLU), tama√±o de lote de 30.  
- `loops_clasificador.py`: C√≥digo con los loops de entrenamiento y validaci√≥n para el clasificador.  
- `matriz_de_confusi√≥n.py`: Genera y visualiza la matriz de confusi√≥n para evaluar el desempe√±o del clasificador.  
- `clasificador_para_autoencoder_1capa_nolineal.py`: Clasificador que utiliza representaciones generadas por un autoencoder de una capa no lineal.  
- `clasificador_sin_preentrenar_1.py` y `c√≥digo_completo_clasificador_sin_preentrenar.py`: Clasificador desarrollado sin preentrenamiento con autoencoders.  

##### Arquitecturas de Autoencoders üîÑ  
- `arquitectura_1_autoencoder.py`: Autoencoder b√°sico de una sola capa.  
- `arquitectura_2_autoencoder(3_capas).py`: Autoencoder m√°s profundo con tres capas ocultas.  
- `arquitectura_3_autoencoder(1capa).py` y `arquitectura_3_autoencoder(1capa,_no_lineal).py`: Autoencoders de una capa con activaciones no lineales para mejor representaci√≥n de las caracter√≠sticas.  
- `arquitectura_autoencoder_no_lineal_1.py`: Versi√≥n alternativa del autoencoder no lineal.  

#### Resultados üìä  
- **Clasificador con encoder pre-entrenado:** Ofrece una mejor representaci√≥n inicial, acelerando la convergencia en las primeras √©pocas. Sin embargo, para alcanzar el mejor desempe√±o, fue necesario ajustar el modelo para la tarea espec√≠fica de clasificaci√≥n.  
- **Entrenado desde cero:** Aunque requiere m√°s tiempo de entrenamiento, logr√≥ mejores resultados cuando se optimiz√≥ correctamente.  

#### Requisitos üñ•Ô∏è  
- Python 3.x  
- PyTorch  
- Numpy  
- Matplotlib  

#### Bibliograf√≠a üìö  
1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Cap√≠tulo 9, "Convolutional Networks".  
2. G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O‚ÄôReilly Media.  
3. LeCun, Y., Bengio, Y., & Hinton, G. (2015). *Deep Learning*. *Nature*, 521(7553), 436-444.  
4. Stanford University. *CS231n: Convolutional Neural Networks for Visual Recognition*. Disponible en: [CS231n](http://cs231n.stanford.edu/).  
5. Chollet, F. (2017). *Deep Learning with Python*. Manning Publications.  
