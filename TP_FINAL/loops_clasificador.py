# -*- coding: utf-8 -*-
"""Loops_clasificador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_eh1gAxSTNGh0XwQ9TiUGHu-rQu0XV6O
"""

def train_loop(dataloader, model, loss_fn, optimizer, verbose = True):
    model.train()  # Activamos el modo de entrenamiento
    num_samples = len(dataloader.dataset)
    num_batches = len(dataloader)
    sum_batch_avrg_loss = 0
    sum_correct = 0            #CAMBIO ACÁ
    num_processed_samples = 0


    #model = model.to(device)   # ESTO CAMBIE
    #Iteramos sobre los lotes
    for batch, (X,y) in enumerate(dataloader):
      X = X.to(device)
      y = y.to(device)
      batch_size = len(X)
      num_processed_samples += batch_size

      #Calculamos la prediccion del modelo y la funcion de perdida correspondiente
      pred = model(X)
      loss = loss_fn(pred,y)

      #Backpropagamos usando el optimizador atribuido
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      #Calculamos la perdida promedio del batch y lo agregamos a una suma correspondiente
      batch_avrg_loss = loss.item()
      sum_batch_avrg_loss += batch_avrg_loss
      #Numero de predicciones correctas:
      sum_correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      #Reportamos el proceso
      if batch % (num_batches/10) == 0 and verbose:
        current = batch * len(X)
        print(f"loss: {batch_avrg_loss:>7f}  [{current:>5d}/{num_samples:>5d}]")

    avg_loss = sum_batch_avrg_loss/num_batches
    precision = sum_correct/num_samples
    return avg_loss, precision

def valid_loop(dataloader, model, loss_fn):
    # Desactivamos la maquinaria de entrenamiento del modelo
    model.eval()

    # Definimos ciertas constantes
    num_samples = len(dataloader.dataset)
    num_batches = len(dataloader)
    sum_batch_avrg_loss = 0
    sum_correct = 0
    num_processed_samples = 0

    # Movemos el modelo a la GPU si es que está disponible
    #model = model.to(device)                      #NO ESTÁ EN EL CODIGO

    # Para testear, desactivamos el cálculo de gradientes.
    with torch.no_grad():
        # Iteramos sobre lotes (batches)
        for X, y in dataloader:
            # Copiamos las entradas y las salidas al dispositivo de trabajo si es que está disponible
            X = X.to(device)
            y = y.to(device)
            batch_size = len(X)  # Número de muestras en el lote
            num_processed_samples += batch_size

            # Calculamos las predicciones del modelo...
            pred = model(X)
            loss = loss_fn(pred, y)

            # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente.
            batch_avrg_loss = loss.item()
            sum_batch_avrg_loss += batch_avrg_loss
            sum_correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        # Calculamos la pérdida total promedio dividiendo entre el número de batches
        avg_loss = sum_batch_avrg_loss / num_batches
        precision = sum_correct/num_samples

        # Imprimimos el resultado (opcional)
        # print(f"@eval_loop avg_loss={avg_loss:>.8f}")

    return avg_loss, precision